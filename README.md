# neural-network

Implementation of a neural network from scratch with batch normalization, ReLu as activation function, softmax as activation for the output layer and cross entropy as loss function. Applied to cifar-10 dataset.